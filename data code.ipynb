{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149cbb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7104e349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöî PatrolIQ Dataset Builder Started...\n",
      "\n",
      "Fetching records from offset 0\n",
      "Fetching records from offset 50000\n",
      "Fetching records from offset 100000\n",
      "Fetching records from offset 150000\n",
      "Fetching records from offset 200000\n",
      "Fetching records from offset 250000\n",
      "Fetching records from offset 300000\n",
      "Fetching records from offset 350000\n",
      "Fetching records from offset 400000\n",
      "Fetching records from offset 450000\n",
      "\n",
      "Total records fetched: 500000\n",
      "Clean records available: 497793\n",
      "\n",
      "‚úÖ DATASET READY!\n",
      "üìÅ File saved as: PatrolIQ_Chicago_Crime_500K.csv\n",
      "üìä Rows: 497793 | Columns: 13\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "BASE_URL = \"https://data.cityofchicago.org/resource/ijzp-q8t2.json\"\n",
    "OUTPUT_FILE = \"PatrolIQ_Chicago_Crime_500K.csv\"\n",
    "\n",
    "TARGET_ROWS = 500_000\n",
    "PAGE_LIMIT = 50_000\n",
    "\n",
    "IMPORTANT_COLUMNS = [\n",
    "    \"id\",\n",
    "    \"case_number\",\n",
    "    \"date\",\n",
    "    \"primary_type\",\n",
    "    \"description\",\n",
    "    \"location_description\",\n",
    "    \"arrest\",\n",
    "    \"domestic\",\n",
    "    \"district\",\n",
    "    \"ward\",\n",
    "    \"community_area\",\n",
    "    \"latitude\",\n",
    "    \"longitude\"\n",
    "]\n",
    "\n",
    "# ==========================\n",
    "# FETCH DATA\n",
    "# ==========================\n",
    "def fetch_page(offset):\n",
    "    params = {\n",
    "        \"$limit\": PAGE_LIMIT,\n",
    "        \"$offset\": offset\n",
    "    }\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "# ==========================\n",
    "# MAIN LOGIC\n",
    "# ==========================\n",
    "def main():\n",
    "    print(\"üöî PatrolIQ Dataset Builder Started...\\n\")\n",
    "\n",
    "    all_data = []\n",
    "    offset = 0\n",
    "\n",
    "    while len(all_data) < TARGET_ROWS:\n",
    "        print(f\"Fetching records from offset {offset}\")\n",
    "        page = fetch_page(offset)\n",
    "\n",
    "        if not page:\n",
    "            print(\"No more data from API.\")\n",
    "            break\n",
    "\n",
    "        all_data.extend(page)\n",
    "        offset += PAGE_LIMIT\n",
    "\n",
    "        if len(page) < PAGE_LIMIT:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nTotal records fetched: {len(all_data)}\")\n",
    "\n",
    "    # ==========================\n",
    "    # DATAFRAME\n",
    "    # ==========================\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Keep only useful columns\n",
    "    df = df[IMPORTANT_COLUMNS]\n",
    "\n",
    "    # ==========================\n",
    "    # DATA CLEANING (BEST DATA)\n",
    "    # ==========================\n",
    "    df.dropna(subset=[\"primary_type\", \"date\", \"location_description\"], inplace=True)\n",
    "    df.drop_duplicates(subset=[\"case_number\"], inplace=True)\n",
    "\n",
    "    # Convert date\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df.dropna(subset=[\"date\"], inplace=True)\n",
    "\n",
    "    print(f\"Clean records available: {len(df)}\")\n",
    "\n",
    "    # ==========================\n",
    "    # RANDOM SAMPLING\n",
    "    # ==========================\n",
    "    if len(df) > TARGET_ROWS:\n",
    "        df = df.sample(n=TARGET_ROWS, random_state=42)\n",
    "\n",
    "    # ==========================\n",
    "    # SAVE CSV\n",
    "    # ==========================\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(\"\\n‚úÖ DATASET READY!\")\n",
    "    print(f\"üìÅ File saved as: {OUTPUT_FILE}\")\n",
    "    print(f\"üìä Rows: {len(df)} | Columns: {df.shape[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
